{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树简介\n",
    "决策树通过定义一系列的**判断模块**（ decision block ），来决定数据的最终标签（分类问题）。\n",
    "\n",
    "原理： 从训练集数据中抽取出一系列的判断模块，形成决策树，测试数据通过经过判断模块来决定最终落入哪个分类。\n",
    "适用数据类型： 数值型 和 标称型；\n",
    "\n",
    "优点： \n",
    "- 数据形式易于理解，决策依据易于理解，保留数据的内在含义（这一点 KNN 做不到）；\n",
    "- 计算复杂度不高；\n",
    "- 对中间值的缺失不敏感；\n",
    "- 可以处理不相关特征数据；\n",
    "\n",
    "缺点：\n",
    "- 容易产生过拟合；\n",
    "\n",
    "基本实现过程：\n",
    "1. 定义数据特征；\n",
    "2. 利用信息熵的计算公式，对比所有数据特征作为判断模块所使用的数据特征时产生的信息增益，使用信息增益最高的特征作为判断模块的指标；\n",
    "3. 根据当前判断模块所使用的指标的所有值（去重）分别建立分支，并遍历分支：\n",
    "    - 如果分支中的实例都是相同标签，则生成一个终止模块（ terminating block );\n",
    "    - 如果分支中的实例存在不同的标签：\n",
    "        - 如果还有未使用的特征，则重新进行步骤 2；\n",
    "        - 如果没有未使用的特征，则使用实例中数量最多的标签建立终止模块；\n",
    "4. 最终形成的决策树的所有叶子节点均为标签终止模块，决策树模型建立完成；\n",
    "5. 测试数据从决策树的根节点开始进行判断，最终到达一个终止模块，决定其标签；\n",
    "\n",
    "\n",
    "信息熵：度量信源的平均不确定性；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n"
     ]
    }
   ],
   "source": [
    "def create_data():\n",
    "    # 生成数据\n",
    "    training_data = [\n",
    "                        [1,1,\"yes\"],\n",
    "                        [1,1,\"yes\"],\n",
    "                        [1,0,\"no\"],\n",
    "                        [0,1,\"no\"],\n",
    "                        [0,1,\"no\"],\n",
    "                    ]\n",
    "    \n",
    "    feature_name = [\"no surfing\", \"flippers\"]\n",
    "    return training_data, feature_name\n",
    "\n",
    "training_data, feature_names = create_data()\n",
    "print(training_data)\n",
    "\n",
    "# {'flippers': {0: 'no', 1: {'no surfing': {0: 'no', 1: 'yes'}}}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def calculate_shannon_ent(data_set):\n",
    "    # 计算给定数据集的香农熵 \n",
    "    # data_set 为数组形式的数组， data_set 的每一个元素为一个实例，且实例最后一个维度为标签\n",
    "    ret = 0.0\n",
    "\n",
    "    row_count = len(data_set)    \n",
    "    label_val_set = set([item[-1] for item in data_set])\n",
    "    label_val_count_dict = dict()\n",
    "    \n",
    "    for row_data in data_set:\n",
    "        label_val = row_data[-1]\n",
    "        if label_val not in label_val_count_dict:\n",
    "            label_val_count_dict[label_val] = 0\n",
    "        label_val_count_dict[label_val] += 1\n",
    "        \n",
    "    for label_val, label_count in label_val_count_dict.items():\n",
    "        possiblity = float(label_count) / float(row_count)\n",
    "        ret -= possiblity * log(possiblity, 2)\n",
    "        \n",
    "    return ret\n",
    "    \n",
    "# calculate the original shannon_ent for training data\n",
    "print(calculate_shannon_ent(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shannon_ent for feature[no surfing] is [0.9182958340544896]...\n",
      "The shannon_ent for feature[flippers] is [1.0]...\n",
      "The best feature for current is flippers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 'flippers')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_data_set(data_set, feature_index, feature_val):\n",
    "    # 筛选出对应特征为指定特征值的记录并移除对应的特征\n",
    "    ret = list()\n",
    "    \n",
    "    for item in data_set:\n",
    "        if item[feature_index] != feature_val:\n",
    "            continue\n",
    "        else:\n",
    "            sub_item_left = item[:feature_index]\n",
    "            sub_item_right = item[feature_index+1:]\n",
    "            \n",
    "            sub_item_left.extend(sub_item_right)\n",
    "            ret.append(sub_item_left)\n",
    "    return ret\n",
    "\n",
    "# print(training_data)\n",
    "# print(split_data_set(training_data,1,1))\n",
    "            \n",
    "\n",
    "\n",
    "def choose_cur_best_feature(data_set, feature_names):\n",
    "    # 对比当前各个特征作为决策模块后的信息熵，取最高的特征之一作为当前最佳特征\n",
    "    feature_shannon_ent_dict = dict()    \n",
    "    feature_count = len(feature_names)\n",
    "    max_shannon_ent = 0.0\n",
    "    max_shannon_feature_index = 0\n",
    "\n",
    "    best_feature = -1\n",
    "    \n",
    "    for i in range(feature_count):\n",
    "        feature_name = feature_names[i]\n",
    "        cur_feature_shannon_ent = 0.0\n",
    "        feature_val_set = set([ item[i] for item in data_set])\n",
    "        for feature_val in feature_val_set:\n",
    "            sub_data_set = split_data_set(data_set, i, feature_val)\n",
    "            sub_prob = len(sub_data_set)/len(data_set)\n",
    "            sub_shannon_ent = calculate_shannon_ent(sub_data_set)\n",
    "            cur_feature_shannon_ent += sub_prob * sub_shannon_ent\n",
    "        if cur_feature_shannon_ent >= max_shannon_ent:\n",
    "            max_shannon_ent = cur_feature_shannon_ent\n",
    "            best_feature = i\n",
    "        print(\"The shannon_ent for feature[%s] is [%s]...\" % (feature_names[i], sub_shannon_ent))\n",
    "    print(\"The best feature for current is %s\" % feature_names[best_feature])\n",
    "    return best_feature, feature_names[best_feature]\n",
    "\n",
    "\n",
    "choose_cur_best_feature(training_data, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shannon_ent for feature[no surfing] is [0.9182958340544896]...\n",
      "The shannon_ent for feature[flippers] is [1.0]...\n",
      "The best feature for current is flippers\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[no surfing] is [0.0]...\n",
      "The best feature for current is no surfing\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "{'flippers': {0: 'no', 1: {'no surfing': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "feature_name_key = \"feature_name\"\n",
    "decisions_key = \"decisions\"\n",
    "result_key = \"result\"\n",
    "\n",
    "import operator\n",
    "import json\n",
    "def get_majority(data_set):\n",
    "    \"\"\"\n",
    "        获取数据集中比重最大的标签\n",
    "    \"\"\"\n",
    "    label_count_dict = dict()\n",
    "    \n",
    "    for item in data_set:\n",
    "        label = item[-1]\n",
    "        if label not in label_count_dict:\n",
    "            label_count_dict[label] = 0\n",
    "        label_count_dict += 1\n",
    "    \n",
    "    sorted_labels = sorted(data_set.iteritems(), key = operator.itemgetter(1), reverse= True)\n",
    "    return sorted_labels[0][0]\n",
    "    \n",
    "def build_decision_tree(data_set, feature_names):\n",
    "    \"\"\"\n",
    "        构建决策树\n",
    "        {'flippers': {0: 'no', 1: {'no surfing': {0: 'no', 1: 'yes'}}}}\n",
    "    \n",
    "\n",
    "    - 如果当前数据中所有实例的标签相同，直接返回标签；\n",
    "    - 否则：        \n",
    "        - 选择最佳特征，根据最佳特征的取值对数据进行分组；\n",
    "            - 遍历每个分组\n",
    "                - 分组数据内没有特征（当前特征是当前分支唯一特征），则取数据中占大多数的标签作为结果；\n",
    "\n",
    "                \n",
    "    \"\"\"    \n",
    "    label_list = [item[-1] for item in data_set]\n",
    "    if len(set(label_list)) == 1:\n",
    "        # stop splitting when all of the labels are equal.\n",
    "        print(\"All labels are equal...\")\n",
    "        return label_list[0]\n",
    "    elif len(feature_names) == 0:\n",
    "        # stop splitting when there is no more feature.\n",
    "        print(\"No more feature...\")\n",
    "        return get_majority(data_set)\n",
    "        pass\n",
    "    else:\n",
    "        cur_feature_id, cur_feature_name = choose_cur_best_feature(data_set, feature_names)\n",
    "        \n",
    "        cur_feature_vals = [ item[cur_feature_id] for item in data_set]\n",
    "        cur_feature_vals = set(cur_feature_vals)\n",
    "        \n",
    "        ret = {\n",
    "            cur_feature_name : dict()\n",
    "        }\n",
    "        \n",
    "        del feature_names[cur_feature_id]\n",
    "        \n",
    "        for feature_val in cur_feature_vals:\n",
    "            sub_labels = feature_names[:]\n",
    "            ret[cur_feature_name][feature_val] = build_decision_tree(split_data_set(data_set,cur_feature_id,feature_val), sub_labels)\n",
    "\n",
    "        return ret\n",
    "    \n",
    "\n",
    "print(build_decision_tree(training_data, feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": [[\"young\", \"myope\", \"no\", \"reduced\", \"no lense\"], [\"young\", \"myope\", \"no\", \"normal\", \"sof\"], [\"young\", \"myope\", \"yes\", \"reduced\", \"no lense\"], [\"young\", \"myope\", \"yes\", \"normal\", \"har\"], [\"young\", \"hyper\", \"no\", \"reduced\", \"no lense\"], [\"young\", \"hyper\", \"no\", \"normal\", \"sof\"], [\"young\", \"hyper\", \"yes\", \"reduced\", \"no lense\"], [\"young\", \"hyper\", \"yes\", \"normal\", \"har\"], [\"pre\", \"myope\", \"no\", \"reduced\", \"no lense\"], [\"pre\", \"myope\", \"no\", \"normal\", \"sof\"], [\"pre\", \"myope\", \"yes\", \"reduced\", \"no lense\"], [\"pre\", \"myope\", \"yes\", \"normal\", \"har\"], [\"pre\", \"hyper\", \"no\", \"reduced\", \"no lense\"], [\"pre\", \"hyper\", \"no\", \"normal\", \"sof\"], [\"pre\", \"hyper\", \"yes\", \"reduced\", \"no lense\"], [\"pre\", \"hyper\", \"yes\", \"normal\", \"no lense\"], [\"presbyopic\", \"myope\", \"no\", \"reduced\", \"no lense\"], [\"presbyopic\", \"myope\", \"no\", \"normal\", \"no lense\"], [\"presbyopic\", \"myope\", \"yes\", \"reduced\", \"no lense\"], [\"presbyopic\", \"myope\", \"yes\", \"normal\", \"har\"], [\"presbyopic\", \"hyper\", \"no\", \"reduced\", \"no lense\"], [\"presbyopic\", \"hyper\", \"no\", \"normal\", \"sof\"], [\"presbyopic\", \"hyper\", \"yes\", \"reduced\", \"no lense\"], [\"presbyopic\", \"hyper\", \"yes\", \"normal\", \"no lense\"]]}\n",
      "['f-0', 'f-1', 'f-2', 'f-3']\n",
      "The shannon_ent for feature[f-0] is [1.061278124459133]...\n",
      "The shannon_ent for feature[f-1] is [1.188721875540867]...\n",
      "The shannon_ent for feature[f-2] is [0.9798687566511527]...\n",
      "The shannon_ent for feature[f-3] is [1.5545851693377994]...\n",
      "The best feature for current is f-0\n",
      "The shannon_ent for feature[f-1] is [0.8112781244591328]...\n",
      "The shannon_ent for feature[f-2] is [1.0]...\n",
      "The shannon_ent for feature[f-3] is [1.5]...\n",
      "The best feature for current is f-1\n",
      "The shannon_ent for feature[f-2] is [1.0]...\n",
      "The shannon_ent for feature[f-3] is [1.0]...\n",
      "The best feature for current is f-2\n",
      "The shannon_ent for feature[f-3] is [0.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-3] is [0.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-2] is [1.0]...\n",
      "The shannon_ent for feature[f-3] is [1.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-2] is [0.0]...\n",
      "The best feature for current is f-2\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-1] is [1.5]...\n",
      "The shannon_ent for feature[f-2] is [1.0]...\n",
      "The shannon_ent for feature[f-3] is [1.0]...\n",
      "The best feature for current is f-1\n",
      "The shannon_ent for feature[f-2] is [1.0]...\n",
      "The shannon_ent for feature[f-3] is [1.0]...\n",
      "The best feature for current is f-2\n",
      "The shannon_ent for feature[f-3] is [0.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-3] is [0.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-2] is [1.0]...\n",
      "The shannon_ent for feature[f-3] is [1.0]...\n",
      "The best feature for current is f-2\n",
      "The shannon_ent for feature[f-3] is [0.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-3] is [0.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-1] is [0.8112781244591328]...\n",
      "The shannon_ent for feature[f-2] is [0.8112781244591328]...\n",
      "The shannon_ent for feature[f-3] is [1.5]...\n",
      "The best feature for current is f-2\n",
      "The shannon_ent for feature[f-1] is [0.0]...\n",
      "The shannon_ent for feature[f-3] is [1.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-1] is [0.0]...\n",
      "The best feature for current is f-1\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-1] is [1.0]...\n",
      "The shannon_ent for feature[f-3] is [1.0]...\n",
      "The best feature for current is f-3\n",
      "All labels are equal...\n",
      "The shannon_ent for feature[f-1] is [0.0]...\n",
      "The best feature for current is f-1\n",
      "All labels are equal...\n",
      "All labels are equal...\n",
      "{'f-0': {'pre': {'f-1': {'myope': {'f-2': {'yes': {'f-3': {'reduced': 'no lense', 'normal': 'har'}}, 'no': {'f-3': {'reduced': 'no lense', 'normal': 'sof'}}}}, 'hyper': {'f-3': {'reduced': 'no lense', 'normal': {'f-2': {'yes': 'no lense', 'no': 'sof'}}}}}}, 'young': {'f-1': {'myope': {'f-2': {'yes': {'f-3': {'reduced': 'no lense', 'normal': 'har'}}, 'no': {'f-3': {'reduced': 'no lense', 'normal': 'sof'}}}}, 'hyper': {'f-2': {'yes': {'f-3': {'reduced': 'no lense', 'normal': 'har'}}, 'no': {'f-3': {'reduced': 'no lense', 'normal': 'sof'}}}}}}, 'presbyopic': {'f-2': {'yes': {'f-3': {'reduced': 'no lense', 'normal': {'f-1': {'myope': 'har', 'hyper': 'no lense'}}}}, 'no': {'f-3': {'reduced': 'no lense', 'normal': {'f-1': {'myope': 'no lense', 'hyper': 'sof'}}}}}}}}\n"
     ]
    }
   ],
   "source": [
    "file_name = \"lenses.txt\"\n",
    "file_handler = open(file_name)\n",
    "\n",
    "data = file_handler.readlines()\n",
    "row_count = len(data)\n",
    "\n",
    "data_set = list()\n",
    "labels = list()\n",
    "\n",
    "for item in data:\n",
    "    item = item[:-2]\n",
    "    data_set.append(item.split(\"\\t\"))\n",
    "\n",
    "for i in range(len(data_set[0])-1):\n",
    "    labels.append(\"f-%s\" % i)\n",
    "\n",
    "print(json.dumps({\"data\" : data_set}, ensure_ascii = False))\n",
    "print(labels)\n",
    "\n",
    "print(build_decision_tree(data_set, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
